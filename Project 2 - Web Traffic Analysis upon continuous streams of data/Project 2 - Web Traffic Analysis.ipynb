{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Web Traffic Analysis\n",
    "\n",
    "### _by Sebastian Sbirna (s190553), Yingrui Li (s171353) and Aijie Shu (s182190)_\n",
    "---\n",
    "\n",
    "**This is the second of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project your task is to analyze a stream of log entries. A log entry consists of an [IP address](https://en.wikipedia.org/wiki/IP_address) and a [domain name](https://en.wikipedia.org/wiki/Domain_name). For example, a log line may look as follows:\n",
    "\n",
    "`192.168.0.1 somedomain.dk`\n",
    "\n",
    "One log line is the result of the event that the domain name was visited by someone having the corresponding IP address. Your task is to analyze the traffic on a number of domains. Counting the number of unique IPs seen on a domain doesn't correspond to the exact number of unique visitors, but it is a good estimate.\n",
    "\n",
    "Specifically, you should answer the following questions from the stream of log entries.\n",
    "\n",
    "- How many unique IPs are there in the stream?\n",
    "- How many unique IPs are there for each domain?\n",
    "- How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "\n",
    "**The answers to these questions can be approximate!**\n",
    "\n",
    "You should also try to answer one or more of the following, more advanced, questions. The answers to these should also be approximate.\n",
    "\n",
    "- How many unique IPs are there for the domains $d_1, d_2, \\ldots$?\n",
    "- How many times was IP X seen on domains $d_1, d_2, \\ldots$?\n",
    "- What are the X most frequent IPs in the stream?\n",
    "\n",
    "You should use algorithms and data structures that you've learned about in the lectures, and you should provide your own implementations of these.\n",
    "\n",
    "Furthermore, you are expected to:\n",
    "\n",
    "- Document the accuracy of your answers when using algorithms that give approximate answers\n",
    "- Argue why you are using certain parameters for your data structures\n",
    "\n",
    "This notebook is in three parts. In the first part you are given an example of how to read from the stream (which for the purpose of this project is a remote file). In the second part you should implement the algorithms and data structures that you intend to use, and in the last part you should use these for analyzing the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the stream\n",
    "The following code reads a remote file line by line. It is wrapped in a generator to make it easier to extend. You may modify this if you want to, but your solution should remain parametrized, so that your notebook can be run without having to consume the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import mmh3\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def stream(n):\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic_2?linkToken=_DcyO-U3MjjuNzI-&itemName=traffic_2') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\")\n",
    "            yield element\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we write a function that will retrieve a new stream of the specified size (i.e. `STREAM_SIZE`). \n",
    "\n",
    "More specifically, it is a generator object will efficiently retrieve the first `STREAM_SIZE` elements from the given URL (opened in the function above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_SIZE = 1000000\n",
    "def get_stream():\n",
    "    web_traffic_stream = stream(STREAM_SIZE)\n",
    "    \n",
    "    return web_traffic_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1, 2 & 4\n",
    "**For Questions 1, 2 and 4**, we have identified that the problems could be solved efficiently by using one or more Hyperloglog count computations, which should drastically reduce size and memory requrirements necessary for computing the number of distinct elements within a string.\n",
    "\n",
    "Since we will need to store the different paramenters of every HLL (hyperloglog) function separate from one another, we have created a simple HLL class, which can be instantiated just as any other object in Python, since we have created its `__init__` function, which will automatically be called when a new object of type HLL is created. The methods for this object will be exactly the ones necessary to update the buckets of the HLL which store the highest number of zeros that was found in any hashed element that passed through it (_add function_), and also to retrieve, starting from this number, the number of unique elements that have passed through the HLL (_count function_), with a mean error of 1.625%, and up to ~4.875% in rare situations.\n",
    "\n",
    "It needs to be mentioned that our implementation of the HLL algorithm uses only one hashing function, however there are other implements which use a list of multiple hashing functions. <br> Such implementations are slightly more robust to potential random errors of finding a hash with very large number of zeros in the beginning of the stream.\n",
    "\n",
    "_Error numbers were retrieved from the last thread response from: https://github.com/ascv/HyperLogLog/issues/28_\n",
    "\n",
    "_The algorithm is highly based on the implementation found in the paper __HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm (Flajolet et al.)__, found here: http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf. Other understandings have been gathered from many articles and resources available online._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HLL(object):\n",
    "    \n",
    "    # Initialization\n",
    "    def __init__(self):\n",
    "        self.p = 9  # p is the precision argument, i.e. the number of bits\n",
    "        self.m = 2 ** self.p  # m is the number of registers in the array M (i.e. the number of buckets)\n",
    "\n",
    "        if self.p == 4:\n",
    "            self.alpha = 0.673\n",
    "        elif self.p == 5:\n",
    "            self.alpha = 0.697\n",
    "        elif self.p == 6:\n",
    "            self.alpha = 0.709\n",
    "        else:\n",
    "            self.alpha = 0.7213 / (1.0 + 1.079 / self.m)\n",
    "\n",
    "        self.M = [0 for _ in range(self.m)]  # initialize the array M with m registers (i.e. buckets)\n",
    "        self.E = 0  # initialize the variable to hold the result\n",
    "\n",
    "    # Aggregation\n",
    "    # Add the element to the set represented by this HyperLogLog.\n",
    "    def add(self, element):\n",
    "        hashedValue = mmh3.hash(str(element))\n",
    "        binValue = '{0:032b}'.format(hashedValue).replace(\"-\", \"0\")  # after\n",
    "\n",
    "        # divide the binary number into the 2 parts, index and value\n",
    "        idx = binValue[len(str(binValue)) - self.p:]\n",
    "        w = binValue[:len(str(binValue)) - self.p]\n",
    "\n",
    "        leadingzeros = self.countzeros(w)  # count the number of leading 0's\n",
    "        idx_int = int(idx, 2)  # convert the index to base 10\n",
    "        self.M[idx_int] = max(self.M[idx_int], leadingzeros)  # compare to number in register and insert the biggest one\n",
    "\n",
    "    # Count the number of leading 0s in a string\n",
    "    def countzeros(self, input):\n",
    "        str_input = str(input)\n",
    "        cnt = 0\n",
    "        for index in range(0, len(str_input) - 1):\n",
    "            if str_input[index] == \"0\":\n",
    "                cnt += 1\n",
    "            else:\n",
    "                break\n",
    "        return cnt\n",
    "\n",
    "    # Computation\n",
    "    # Should return an estimate of the current number of (distinct) elements in the set.\n",
    "    def count(self):\n",
    "        E = self.alpha * float(self.m ** 2) / sum(math.pow(2.0, -x) for x in self.M)\n",
    "\n",
    "        # V = self.M.count(0)\n",
    "        V = 0\n",
    "        for i in range(len(self.M)):\n",
    "            if self.M[i] == 0:\n",
    "                V += 1\n",
    "\n",
    "        if E <= (5.0 / 2.0) * float(self.m):\n",
    "            if V != 0:\n",
    "                E = self.linearCounting(self.m, V)\n",
    "            else:\n",
    "                E = E\n",
    "        elif E <= (1.0 / 30) * (2 ** 32):\n",
    "            E = E\n",
    "        else:\n",
    "            E = (-2 ** 32) * math.log(1.0 - (E / (2 ** 32)))\n",
    "        return int(E)\n",
    "\n",
    "    def linearCounting(self, m, V):\n",
    "        return m * math.log(float(m) / float(V))  # the same as m * math.log(m/V)\n",
    "\n",
    "    # Should return a new hyperloglog that corresponds to the union(merge) of this HLL object and another (i.e. \"other\").\n",
    "    def __add__(self, other):\n",
    "        h = HLL()\n",
    "        for i in range(self.m):\n",
    "            h.M[i] = max(self.M[i], other.M[i])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 & 5\n",
    "**For Questions 3 and 5,** if the X, Y are given during compiling time, the frequency of X given Y(s) can be solved by one simple counter. In this question, we assume the X, Y are specified at an arbitray time during the stream is processing. Therefore, the **Count Min Sketch** is used here because it records the counts for any given X.\n",
    "\n",
    "As streaming data could only be read once, we need to record the amount of times that one IP passes by. Known the name of a specific IP address, we use different hash functions to map it to a table where width is the linear space for all the hashed IP coming in, depth is the number of functions we utilize. The fact we can not ignore is: several IP addresses could be mapped into one place where collisions happen. Thus, using hash functions will **almost always overestimate** the amount of IP. Hence, we need numerous hash functions to get different frequency, picking the minimum one will obviously give us the most precise estimate overall.\n",
    "\n",
    "### Reason of bad performance of Count Min Sketch\n",
    "\n",
    "Truly, the top 10 most frequent IPs in the first 1000000 pieces of data in stream are:\n",
    "\n",
    "['72.187.84.158', '108.41.112.108', '204.141.72.187', '53.30.199.128', '55.29.199.128', '56.29.200.127', '55.31.199.127', '54.30.199.128', '53.32.200.127', '53.29.198.127']\n",
    "    \n",
    "Relevant frequency is:\n",
    "\n",
    "[222, 199, 139, 34, 34, 33, 32, 31, 30, 30]\n",
    "\n",
    "In our case, <font color='red'>the frequency of IPs doesn't varies so much shown in accuracy test on Question 6 on 1 million IPs. In other words, count mean sketch algorithm is not expected to show a good performance because hash collision affects the low-frequency elements a lot.</font> Therefore, the **Count Mean Min Sketch** algorithm is introduced to improve the accuracy on the long run. In Count Mean Min Sketch algorithm, the noise caused by hash collision is deducted and minimum-based picking is replaced by median-based picking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class CountMinSketch(object):\n",
    "    \"\"\"\n",
    "    `w`: Width(size) of table.\n",
    "    `d`: Depth of table, which is the number of hash functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w=9919, d=10):\n",
    "        self.w = w\n",
    "        self.d = d\n",
    "        self.counts = [array('L', (0 for _ in range(self.w))) for _ in range(self.d)]\n",
    "        self.hash_functions = [lambda x: mmh3.hash(x, seed = k) for k in range(self.d)]\n",
    "        \n",
    "    def get_columns(self, a):\n",
    "        for hash_i in self.hash_functions:\n",
    "            yield hash_i(a) % self.w\n",
    "        \n",
    "    def update(self, a, val=1):\n",
    "        for row, col in zip(self.counts, self.get_columns(a)):\n",
    "            row[col] += val\n",
    "    \n",
    "    def query(self, a):\n",
    "        return min(row[col] for row, col in zip(self.counts, self.get_columns(a)))\n",
    "    \n",
    "    def __getitem__(self, a):\n",
    "        return self.query(a)\n",
    "    \n",
    "    def __setitem__(self, a, val):\n",
    "        for row, col in zip(self.counts, self.get_columns(a)):\n",
    "            row[col] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class CountMeanMinSketch(object):\n",
    "    \"\"\"\n",
    "    `w`: Width(size) of table.\n",
    "    `d`: Depth of table, which is the number of hash functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, w=9919, d=10):\n",
    "        self.w = w\n",
    "        self.d = d\n",
    "        self.counts = [array('L', (0 for _ in range(self.w))) for _ in range(self.d)]\n",
    "        self.hash_functions = [lambda x: mmh3.hash(x, seed = k) for k in range(self.d)]\n",
    "        \n",
    "    def get_columns(self, a):\n",
    "        for hash_i in self.hash_functions:\n",
    "            yield hash_i(a) % self.w\n",
    "        \n",
    "    def update(self, a, val=1):\n",
    "        for row, col in zip(self.counts, self.get_columns(a)):\n",
    "            row[col] += val\n",
    "    \n",
    "    def query(self, a):\n",
    "        hash_vals_a = []\n",
    "        for row, col in zip(self.counts, self.get_columns(a)):\n",
    "            noise = np.mean(row)#(np.sum(row) - row[col]) / (self.w - 1)         # calculate noise on all elements of that row except row[col]\n",
    "            hash_vals_a.append(row[col] - noise)                    # supercede noise\n",
    "        return int(round(np.median(hash_vals_a)))                   # get the median\n",
    "\n",
    "    def __getitem__(self, a):\n",
    "        return self.query(a)\n",
    "    \n",
    "    def __setitem__(self, a, val):\n",
    "        for row, col in zip(self.counts, self.get_columns(a)):\n",
    "            row[col] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**For Question 6**, a Min Heap in which the IP with minimum frequency among the X most frequently IP addresses is maintained. The algorithm is described as below.\n",
    "1. With the incoming stream, the Count Mean Min Sketch keep updating the 2D array (width x depth).\n",
    "2. Maintain a min heap with length k\n",
    "3. Each time a new IP stream in, the sketch increase 1. \n",
    "4. - If the new IP has been recorded by the heap, the frequency of that IP in the heap increase by 1, then update the min heap in order to ensure the IP with the minimum frequency among X most frequent IPs is on the root node.\n",
    "   - If the new IP haven't been encountered before, compare its frequency with the root node of min heap. \n",
    "       - If the frequency of new IP is greater than the frequency of root node, replace the root node and update the heap. Otherwise, skip this IP and check next new IP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remind that,** for the `https://files.dtu.dk/fss/public/link/public/stream/read/traffic_2?linkToken=_DcyO-U3MjjuNzI-&itemName=traffic_2` data stream, the Count Mean Min Sketch <font color='red'>also do not performs very well</font> because the low frequency of X most frequently IPs. So in the future, Lossy Counting and Space Saving algorithm can be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q1: How many unique IPs are there in the stream?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q1_retrieve_unique_ip_count(web_traffic_stream):\n",
    "    hll = HLL()                                        # Instantiate 1 HLL object, which will estimate the number of unique IPs that we are passing through it\n",
    "\n",
    "    for entry in web_traffic_stream:\n",
    "        c_ip = entry.split('\\t')[0]                    # Here we store the IP of the entry\n",
    "        c_domain = entry.split('\\t')[1]                # Here we store the web domain of the entry\n",
    "        hll.add(c_ip)                                  # We update the HLL object by calling its 'add' function, which passes a new entry through it and updates its buckets\n",
    "        \n",
    "    return hll.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total count of unique IPs in the stream: 944333\n"
     ]
    }
   ],
   "source": [
    "approx_unique_ip_count = Q1_retrieve_unique_ip_count(get_stream())\n",
    "\n",
    "print('Estimated total count of unique IPs in the stream: {}'.format(approx_unique_ip_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact count of the unique number of IPs in the stream: 971745\n"
     ]
    }
   ],
   "source": [
    "## This is a test for computing the accuracy of the algorithm.\n",
    "## We are using pandas DataFrames for performing quick computations of the exact number requested by the question (Q1) \n",
    "## Saving data in memory through DataFrame is not efficient, and this is only meant to be seen as a benchmarking and testing field, which would not exist in a \"production\" environment\n",
    "\n",
    "web_traffic_stream = get_stream()\n",
    "\n",
    "# Create a DataFrame for storing the traffic data, and split the entries into IP column and Domain column by their tab separator\n",
    "traffic_df = pd.DataFrame(list(s.split('\\t') for s in web_traffic_stream), columns = ['ip', 'domain'])\n",
    "\n",
    "# Compute the exact number of unique IPs in the stream (by using the DataFrame):\n",
    "true_unique_ip_count = len(traffic_df.ip.unique())\n",
    "\n",
    "print('Exact count of the unique number of IPs in the stream: {}'.format(true_unique_ip_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference between HLL algorithm and real-world answer: 2.821%\n"
     ]
    }
   ],
   "source": [
    "print('Error difference between HLL algorithm and real-world answer: {}%'\n",
    "      .format(abs(round(((1 - (approx_unique_ip_count / true_unique_ip_count)) * 100), 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q2: How many unique IPs are there for each domain?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q2_retrieve_unique_ip_per_domain(web_traffic_stream):\n",
    "\n",
    "    # Since we can expect that the total number of popular domains visited is going to be similar across people, the number of domains encountered is expected to be quite small\n",
    "    # Therefore, it is computationally reasonable to say that keeping a dictionary with the keys as being the unique domains is computationally efficient\n",
    "    # The values in this dictionary will be a Hyperloglog object for each unique key (i.e. domain), so that we can count how many unique IPs are found for each individual domain\n",
    "\n",
    "    domains_dict = {} # Initialize the domain dictionary\n",
    "\n",
    "    for entry in web_traffic_stream:\n",
    "        c_ip = entry.split('\\t')[0]\n",
    "        c_domain = entry.split('\\t')[1]\n",
    "\n",
    "        if (not domains_dict.get(c_domain)):                        # If the key does not yet exist in the domain dictionary, \n",
    "            domains_dict[c_domain] = HLL()                          # Add the new domain to the dictionary, and set its value to be a new hyperloglog object\n",
    "        domains_dict[c_domain].add(c_ip)                            # We update the HLL object of a corresponding domain, by calling its 'add' function upon each IP that visited the domain\n",
    "\n",
    "    for domain, hll in domains_dict.items():                        # For each individual domain, we will print the unique number of items counted by the corresponding HLL object\n",
    "        print('Domain: {}, approximate count: {}'.format(domain, hll.count()))\n",
    "        \n",
    "    return domains_dict                                             # The only purpose why we return the domain dictionary is to be able to perform error benckmarking on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: python.org, approximate count: 265174\n",
      "Domain: wikipedia.org, approximate count: 480675\n",
      "Domain: pandas.pydata.org, approximate count: 120872\n",
      "Domain: dtu.dk, approximate count: 26610\n",
      "Domain: google.com, approximate count: 25719\n",
      "Domain: databricks.com, approximate count: 13129\n",
      "Domain: github.com, approximate count: 12531\n",
      "Domain: spark.apache.org, approximate count: 5104\n",
      "Domain: datarobot.com, approximate count: 2762\n",
      "Domain: scala-lang.org, approximate count: 1\n"
     ]
    }
   ],
   "source": [
    "# In domains domains_dict, we will save the HLL object for each domain, so we can reuse it when computing error benchmarking\n",
    "domains_dict = Q2_retrieve_unique_ip_per_domain(get_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: python.org, true unique IP count: 256386\n",
      "(Error difference: 3.428%\n",
      "Domain: wikipedia.org, true unique IP count: 510191\n",
      "(Error difference: 5.785%\n",
      "Domain: pandas.pydata.org, true unique IP count: 128723\n",
      "(Error difference: 6.099%\n",
      "Domain: dtu.dk, true unique IP count: 26144\n",
      "(Error difference: 1.782%\n",
      "Domain: google.com, true unique IP count: 26082\n",
      "(Error difference: 1.392%\n",
      "Domain: databricks.com, true unique IP count: 13143\n",
      "(Error difference: 0.107%\n",
      "Domain: github.com, true unique IP count: 12788\n",
      "(Error difference: 2.01%\n",
      "Domain: spark.apache.org, true unique IP count: 5217\n",
      "(Error difference: 2.166%\n",
      "Domain: datarobot.com, true unique IP count: 2559\n",
      "(Error difference: 7.933%\n",
      "Domain: scala-lang.org, true unique IP count: 1\n",
      "(Error difference: 0.0%\n"
     ]
    }
   ],
   "source": [
    "for domain in traffic_df.domain.unique():\n",
    "    approx_unique_ip_nr_by_domain = domains_dict[domain].count()\n",
    "    true_unique_ip_nr_by_domain = len(traffic_df[traffic_df.domain == domain].ip.unique())\n",
    "    \n",
    "    print('Domain: {}, true unique IP count: {}'.format(domain, true_unique_ip_nr_by_domain))\n",
    "    print('(Error difference: {}%'.format(abs(round(((1 - (approx_unique_ip_nr_by_domain / true_unique_ip_nr_by_domain)) * 100),3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q3: How many times was IP X seen on domain Y? (for some X and Y provided at run time)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3_count_IP_X_in_domain_Y(web_traffic_stream, X, Y):\n",
    "    domains_dict = {}\n",
    "    \n",
    "    for entry in web_traffic_stream:\n",
    "        \n",
    "        c_ip = entry.split('\\t')[0]\n",
    "        c_domain = entry.split('\\t')[1]\n",
    "        \n",
    "        if (not domains_dict.get(c_domain)): # If the domain does not yet exist in the domain dictionary, \n",
    "            domains_dict[c_domain] = CountMeanMinSketch(w = 49999, d = 10)   # Add the new domain to the dictionary, and set its value to be a new hyperloglog object\n",
    "        domains_dict[c_domain].update(c_ip)\n",
    "        \n",
    "    return domains_dict[Y].query(X)                    # Returns an approximate count of IP X in domain Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total count of IP \"54.27.201.128\" in domain \"python.org\" is: 8\n"
     ]
    }
   ],
   "source": [
    "ip_X = '54.27.201.128'       # Here we can change the IP value to be searched for\n",
    "domain_Y = 'python.org'      # Here we can change the domain to be searched into\n",
    "\n",
    "approx_count_X_in_Y = Q3_count_IP_X_in_domain_Y(get_stream(), ip_X, domain_Y)\n",
    "\n",
    "print('Estimated total count of IP \"{}\" in domain \"{}\" is: {}'.format(ip_X, domain_Y, approx_count_X_in_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact total count of IP \"54.27.201.128\" in domain \"python.org\" is: 6\n"
     ]
    }
   ],
   "source": [
    "true_count_X_in_Y = len(traffic_df[(traffic_df.ip == ip_X) & (traffic_df.domain == domain_Y)])\n",
    "\n",
    "print('Exact total count of IP \"{}\" in domain \"{}\" is: {}'.format(ip_X, domain_Y, true_count_X_in_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference between CountMinSketch and real-world answer: 33.333%\n"
     ]
    }
   ],
   "source": [
    "print('Error difference between CountMinSketch and real-world answer: {}%'\n",
    "      .format(abs(round(((1 - (approx_count_X_in_Y / true_count_X_in_Y)) * 100),3)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q4: How many unique IPs are there for the domains $d_1$, $d_2$, … ?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4_unique_ip_joint_count_in_domains(web_traffic_stream, list_of_domains):\n",
    "    domains_dict = {}                            # Same reasoning for a domain dictionary as in Q2\n",
    "\n",
    "    for entry in web_traffic_stream:\n",
    "        c_ip = entry.split('\\t')[0]\n",
    "        c_domain = entry.split('\\t')[1]\n",
    "\n",
    "        if (c_domain in list_of_domains):        # If domain of the entry is within the ones that we are interested in,\n",
    "            if (not domains_dict.get(c_domain)): # If the key does not yet exist in the domain dictionary, \n",
    "                domains_dict[c_domain] = HLL()   # Add the new domain to the dictionary, and set its value to be a new hyperloglog object\n",
    "                \n",
    "            domains_dict[c_domain].add(c_ip)     # We update the HLL object of a corresponding domain, by calling its 'add' function upon each IP that visited the domain\n",
    "            \n",
    "    joint_hll_obj = None\n",
    "    \n",
    "    for domain, hll in domains_dict.items():     # For each individual domain, we will merge its HLL object with the previous ones found\n",
    "        if (joint_hll_obj == None):\n",
    "            joint_hll_obj = hll\n",
    "        joint_hll_obj = joint_hll_obj + hll      # We will merge together the hyperloglog buckets, as implemented in the __add__ function (which also allows us to use the \"+\" sign for merging)\n",
    "        \n",
    "    approx_joint_unique_ip_count = joint_hll_obj.count()\n",
    "    return approx_joint_unique_ip_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approximate number of total unique IPs within the domain list ['python.org', 'wikipedia.org'] is: 740003\n"
     ]
    }
   ],
   "source": [
    "list_of_domains = ['python.org', 'wikipedia.org']      # Here we can change the list of domains to be searched into\n",
    "\n",
    "approx_joint_unique_ip_count = Q4_unique_ip_joint_count_in_domains(get_stream(), list_of_domains)\n",
    "\n",
    "print('The approximate number of total unique IPs within the domain list {} is: {}'\n",
    ".format(list_of_domains, approx_joint_unique_ip_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact count of the total unique IPs within the domain list ['python.org', 'wikipedia.org'] is: 762628\n"
     ]
    }
   ],
   "source": [
    "true_joint_unique_ip_count = len(traffic_df[traffic_df.domain.isin(list_of_domains) == True].ip.unique())\n",
    "\n",
    "print('Exact count of the total unique IPs within the domain list {} is: {}'.format(list_of_domains, true_joint_unique_ip_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference between HLL algorithm and real-world answer: 2.967%\n"
     ]
    }
   ],
   "source": [
    "print('Error difference between HLL algorithm and real-world answer: {}%'\n",
    "      .format(abs(round(((1 - (approx_joint_unique_ip_count / true_joint_unique_ip_count)) * 100),3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q5: How many times was IP X seen on domains $d_1$, $d_2$, … ?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q5_joint_count_IP_X_in_domains(web_traffic_stream, X, list_of_domains):\n",
    "    domains_dict = {}                                      \n",
    "    \n",
    "    for entry in web_traffic_stream:\n",
    "        c_ip = entry.split('\\t')[0]\n",
    "        c_domain = entry.split('\\t')[1]\n",
    "\n",
    "        if (not domains_dict.get(c_domain)): \n",
    "            domains_dict[c_domain] = CountMeanMinSketch(w = 49999, d = 10)   \n",
    "        domains_dict[c_domain].update(c_ip)  \n",
    "        \n",
    "    approx_joint_count_of_X = 0\n",
    "    for domain in list_of_domains:\n",
    "        approx_joint_count_of_X += domains_dict[domain].query(X)            # sum up the minimum number of counts that IP X was seen in each of the separate domains\n",
    "    \n",
    "    return approx_joint_count_of_X\n",
    "    return approx_joint_count_of_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total count of IP \"54.27.201.128\" in list of domains ['python.org', 'wikipedia.org', 'dtu.dk'] is: 10\n"
     ]
    }
   ],
   "source": [
    "ip_X = '54.27.201.128'                                           # Here we can change the IP value to be searched for \n",
    "list_of_domains = ['python.org', 'wikipedia.org', 'dtu.dk']      # Here we can change the list of domains to be searched into\n",
    "\n",
    "approx_joint_count_of_X = Q5_joint_count_IP_X_in_domains(get_stream(), ip_X, list_of_domains)\n",
    "\n",
    "print('Estimated total count of IP \"{}\" in list of domains {} is: {}'.format(ip_X, list_of_domains, approx_joint_count_of_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q5_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact total count of IP \"54.27.201.128\" in list of domains ['python.org', 'wikipedia.org', 'dtu.dk'] is: 10\n"
     ]
    }
   ],
   "source": [
    "true_joint_count_of_X = len(traffic_df[(traffic_df.domain.isin(list_of_domains) == True) & (traffic_df.ip == ip_X)])\n",
    "\n",
    "print('Exact total count of IP \"{}\" in list of domains {} is: {}'.format(ip_X, list_of_domains, true_joint_count_of_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference between CountMin algorithm and real-world answer: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print('Error difference between CountMin algorithm and real-world answer: {}%'\n",
    "      .format(abs(round(((1 - (approx_joint_count_of_X / true_joint_count_of_X)) * 100),3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Q6: What are the X most frequent IPs in the stream?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def Q6_X_most_frequent_IPs(web_traffic_stream, X):\n",
    "    heap = []          # [freq, ip]\n",
    "    top_k_dict = {}   # ip:[freq, ip]\n",
    "    \n",
    "    countmin = CountMinSketch(w = 49999, d = 10)\n",
    "    \n",
    "    for entry in web_traffic_stream:\n",
    "        ip = entry.split('\\t')[0]\n",
    "        countmin.update(ip)\n",
    "        \n",
    "        new_freq = countmin.query(ip) \n",
    "        \n",
    "        if ip in top_k_dict:                              # if ip has been recorded in top k dict\n",
    "            top_k_dict[ip] = new_freq                     # update the dict\n",
    "            for i in heap:                                # update the heap\n",
    "                if i[1] == ip:\n",
    "                    i[0] = new_freq\n",
    "            heapq.heapify(heap)                           # rebalance the heap\n",
    "        elif len(top_k_dict) < X:                         # if the heap is not full\n",
    "            heapq.heappush(heap, [new_freq, ip])\n",
    "            top_k_dict[ip] = [new_freq, ip]\n",
    "        elif new_freq > heap[0][0]:                       # if the freq of new ip larger than the root node of the heap\n",
    "            old_freq = heapq.heappushpop(heap, [new_freq, ip]) # replace\n",
    "            del top_k_dict[old_freq[1]]\n",
    "            top_k_dict[ip] = [new_freq, ip]\n",
    "    X_most = heapq.nlargest(X, heap)\n",
    "    \n",
    "    return [x[1] for x in X_most], [x[0] for x in X_most]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately, the top 10 most frequent IPs in the stream are:\n",
      "['72.187.84.158', '204.42.88.167', '199.104.181.134', '122.109.77.148', '156.145.82.189', '108.41.112.108', '253.174.113.76', '207.104.210.170', '125.88.139.101', '188.221.128.150']\n",
      "Relevant frequency is: \n",
      "[248, 236, 234, 232, 225, 220, 217, 213, 213, 211]\n"
     ]
    }
   ],
   "source": [
    "frequency_X = 10                     # Here we can change the number of X most frequent IPs to search for\n",
    "approx_X_most_frequent_IPs = Q6_X_most_frequent_IPs(get_stream(), frequency_X)\n",
    "\n",
    "print('Approximately, the top {} most frequent IPs in the stream are:\\n{}'.format(frequency_X, approx_X_most_frequent_IPs[0]))\n",
    "print('Relevant frequency is: \\n{}'.format(approx_X_most_frequent_IPs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Accuracy test: Q6_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truly, the top 10 most frequent IPs in the stream are: \n",
      "['72.187.84.158', '108.41.112.108', '204.141.72.187', '55.29.199.128', '53.30.199.128', '56.29.200.127', '55.31.199.127', '54.30.199.128', '53.29.198.127', '56.30.200.127']\n",
      "Relevant frequency is:\n",
      "[222, 199, 139, 34, 34, 33, 32, 31, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# We will again use our (inefficient) pandas DataFrame object to perform exact calculations on how many unique IPs are there for the above-specified domain list\n",
    "\n",
    "true_X_most_frequent_IPs = (traffic_df.ip.value_counts().iloc[0:frequency_X].index.tolist())\n",
    "true_X_most_frequent_freqs = (traffic_df.ip.value_counts().iloc[0:frequency_X].tolist())\n",
    "\n",
    "print('Truly, the top {} most frequent IPs in the stream are: \\n{}'.format(frequency_X, true_X_most_frequent_IPs))\n",
    "print('Relevant frequency is:\\n{}'.format(true_X_most_frequent_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error difference between countmin sketch using heap algorithm and real-world answer: 80.0%\n"
     ]
    }
   ],
   "source": [
    "nr_of_common_elements_between_frequency_lists = len(set(true_X_most_frequent_IPs).intersection(approx_X_most_frequent_IPs[0]))\n",
    "\n",
    "print('Error difference between countmin sketch using heap algorithm and real-world answer: {}%'\n",
    "      .format(abs(round(((1 - (nr_of_common_elements_between_frequency_lists / frequency_X)) * 100),3))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
